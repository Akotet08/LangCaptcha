# -*- coding: utf-8 -*-
"""choose_translatoins.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GTLxrv64wTyUKc7w9RKHUjCUGMddsJUG
"""

!pip install transformers
!pip install scikit-learn-extra

from transformers import BertTokenizer, BertModel, AutoModel, AutoTokenizer, BertForMaskedLM
import numpy as np
import torch
from sklearn_extra.cluster import KMedoids

def cal_score_cosine(a, b):
    if len(a.shape) == 1: a = a.unsqueeze(0)
    if len(b.shape) == 1: b = b.unsqueeze(0)

    a_norm = a / a.norm(dim=1)[:, None]
    b_norm = b / b.norm(dim=1)[:, None]
    return torch.mm(a_norm, b_norm.transpose(0, 1)) * 100

def cal_score_L2(a, b):
    if len(a.shape) == 1: a = a.unsqueeze(0)
    if len(b.shape) == 1: b = b.unsqueeze(0)

    dist = torch.norm(a - b, dim=1)
    return dist

def distance_matrix(sentences, model_link='BM-K/KoSimCSE-roberta-multitask'):

  model = AutoModel.from_pretrained(model_link)
  tokenizer = AutoTokenizer.from_pretrained(model_link)

  size = len(sentences)
  dist_mat = np.zeros((size, size))

  inputs = tokenizer(sentences, padding=True, truncation = True, return_tensors='pt')
  embeddings, _ = model(**inputs, return_dict=False)

  for i in range(size):
    for j in range(i):

      # score = cal_score_cosine(embeddings[i][0], embeddings[j][0])
      # dist = 100 - score  # because score is max for the most similar ~ 100

      # wnen considering the L2 metric
      dist = cal_score_L2(embeddings[i][0], embeddings[j][0])

      dist_mat[i][j] = dist
      dist_mat[j][i] = dist

  return dist_mat

def cluster(dist_mat, num_cluster = 3):

  k_medoids = KMedoids( n_clusters = num_cluster, metric = 'precomputed')

  k_medoids.fit(dist_mat)

  labels = k_medoids.labels_
  medoid_indices = k_medoids.medoid_indices_

  return labels, medoid_indices

def predict_word_probability(sentence, masked_index, model, tokenizer):

  token_ids = tokenizer.encode(sentence, return_tensors='pt')
  token_ids[0][masked_index] = tokenizer.mask_token_id

  # Predict the masked word
  with torch.no_grad():
      predictions = model(token_ids).logits

  # Get the probability of the original word
  original_word_id = tokenizer.encode(sentence.split()[masked_index], add_special_tokens=False)[0]
  probability = torch.nn.functional.softmax(predictions[0, masked_index], dim=0)[original_word_id].item()

  return probability

def sentence_probability(sentence, model, tokenizer):
  words = sentence.split()
  probs = []

  for i in range(1, len(words) - 1):  # Skip the first and last word for simplicity
      prob = predict_word_probability(sentence, i, model, tokenizer)
      probs.append(prob)

  return sum(probs) / len(probs)

translations = [
    ["나 우산을 못찾겠어", "네이비드는  젊은 블로거로서, 자신의 삶과 아이디어를 블로그에 공유합니다.", "이 모든것에 불구하고 경찰은 떠나선 안됩니다.", "학교의 일별 방문자 수를 제한하는 정책이 마련되어야 합니다.", "이 이벤트의 주체자는 어디에 있습니까?", "그들은 적절한 재활을 요구합니다."],
    ["나는 내 우산을 찾을 수 없다.", "신입 블로거로서, 나비드는 그의 삶과 생각을 둘 다 그의 블로그에서 공유한다.", "이 모든 것에도 불구하고, 그 경찰은 떠나지 않는다.", "학교에서는 날마다 방문객의 숫자를 제한하는 정책이 있어야 한다.", "이 행사의 주최자가 어디에 있나요?", "그들은 적절한 재활을 요구하고 있다."],
    ["내 우산을 못 찾겠어.", "젊은 블로거로서, 나비드는 그의 삶과 아이디어들 둘 다를 그의 블로그에  공유한다.", "이 모든 것에도 불구히고, 경찰은 떠나지 않았다.", "일일 학교 방뮨자를 제한하는 규칙이 있어야 한다.", "이 행사의 주최자는 어디에 있습니까?", "그들은 적절힌 재활을 요구하고 있다."],
    ["우산을 찾을 수 없어요.", "네이비드는 젊은 블로거로, 삶과 아이디어를 블로그에 공유합니다.", "어떤 일이 있어도, 경찰은 그 자리를 떠나지 않는다.", "하루 학교 방문자 수를 제한하는 정책이 있어야 한다.", "여기 이벤트 주최자 어디 있어요?", "그들은 더 나은 부흥을 원한다."],
    ["나는 내 우산을 찾을 수 없다", "젊은 블로거로서, 네이비드는  그의 삶과 생각 모두 그의 블로그이 공유한다.", "이 모든 일에도 불구하고, 경찰관은 떠나지 않는다.", "학교의 하루 방문자수를 제한하는 방침이 있어야만 한다.", "이 행사의 주촤자는 어디에 있는가?", "그들은 정당한 복직을 요구하고 있습니다"],
    ["제 우산을 찾을 수 없습니다", "젊은 블로거로서, 네이비드는 그의 블로그에서 삶과 생각을 모두  공유합니다.", "이 모든거에도  불구하고, 경찰은 떠나지 않습니다", "하루에 학교 방문객 수를 제한하는 정책이 있어야 합니다", "이 행사의 주최자는 어디에 있습니까?", "그들은 적절한 재활을 요구하고 있습니다"],
    ["우산을 찾을 수 없습니다.", "나비드는 초보 블로거로서 그의 일상과 생각을 블로그에 공유했다", "그럼에도 불구하고 경찰은 떠나지 않았다", "하루에 학교를 방문할 수 있는 방문자 숫자를 제한하는 규칙이 있어야 한다.", "이 행사의 운영자는 누구인가요?", "적절한 재활에 대한 수요가 있다"]
]

reordered = [[0] * len(translations) for _ in range(len(translations[0]))]
for i in range(len(reordered)):
  for j in range(len(reordered[0])):
    reordered[i][j] = translations[j][i]

model_name = 'BM-K/KoSimCSE-roberta-multitask'
model = BertForMaskedLM.from_pretrained(model_name)
tokenizer = BertTokenizer.from_pretrained(model_name)

output = []

i = 0

for sentences in reordered:

  dist_mat = distance_matrix(sentences)
  labels, medoids = cluster(dist_mat, num_cluster = 3)

  label, freq = np.unique(labels, return_counts = True)
  most_populous = label[freq.argmax()]

  # cluster_sentences = [sentences[i] for i in range(len(labels)) if labels[i] == most_populous]


  cluster_sentences = [sentences[i] for i in range(len(sentences)) if labels[i] == most_populous]
  prob_sentences = [sentence_probability(sentence, model, tokenizer) for sentence in cluster_sentences]

  most_probable_sentence = cluster_sentences[np.argmax(prob_sentences)]
  index = sentences.index(most_probable_sentence)



  output.append(index)

  i += 1
  print(i)



output

winning = []
for w in range(len(output)):
  win = output[w]
  pop = reordered[w]
  winning.append(pop[win])

winning

# translations = [
#     ["버스를 기다리는 동안, 몇 년 동안 못 본 오랜 친구를 만났다.",
#      "매 여름, 우리는 짐을 싸고 할머니 집을 방문하기 위해 시골로 갔다.",
#      "처음으로 스시를 시도했을 때의 기억; 호기심과 불안감의 혼합이었다.",
#      "그녀는 창밖을 내다보며 커피를 마시며 다가올 모험에 대한 생각에 잠겼다.",
#      "비에도 불구하고, 거리는 연례 음악 축제에 참석하려는 사람들로 북적였다."],  #1

#     ["버스를 기다리다가 몇 년 만에 본 친구를 우연히 만났어.",
#      "여름이면 가방을 싸서 할머니 집에 놀러 갔다.",
#      "스시를 처음 먹어볼 때, 궁금증과 조심스러움이 있었다.",
#      "그녀는 창가에 앉아 커피를 마시며 미래의 모험을 상상했다.",
#      "비가 오는데도 불구하고, 사람들은 음악 축제에 참여하기 위해 거리로 나왔다."],]

# reordered = [[0] * len(translations) for _ in range(len(translations[0]))]
# for i in range(len(reordered)):
#   for j in range(len(reordered[0])):
#     reordered[i][j] = translations[j][i]

# reordered

# model_name = 'BM-K/KoSimCSE-roberta-multitask'
# model = BertForMaskedLM.from_pretrained(model_name)
# tokenizer = BertTokenizer.from_pretrained(model_name)

# # dist_mat = distance_matrix(reordered[0])
# # cluster(dist_mat, num_cluster = 2)
# # sentence_probability(reordered[0][0], model, tokenizer)

# output = []
# for sentences in reordered:

#   dist_mat = distance_matrix(sentences)
#   labels, medoids = cluster(dist_mat, num_cluster = 2)

#   label, freq = np.unique(labels, return_counts = True)
#   most_populous = label[freq.argmax()]

#   cluster_sentences = [sentences[i] for i in range(len(sentences)) if label[i] == most_populous]
#   prob_sentences = [sentence_probability(sentence, model, tokenizer) for sentence in cluster_sentences]

#   most_probable_sentence = cluster_sentences[np.argmax(prob_sentences)]
#   index = sentences.index(most_probable_sentence)

#   output.append(index)




# output

# # given a list of translation of translations, from different people on a fixed number of inputs
# # here is a gpt generated dataset

# translations = [
#     ["버스를 기다리는 동안, 몇 년 동안 못 본 오랜 친구를 만났다.",
#      "매 여름, 우리는 짐을 싸고 할머니 집을 방문하기 위해 시골로 갔다.",
#      "처음으로 스시를 시도했을 때의 기억; 호기심과 불안감의 혼합이었다.",
#      "그녀는 창밖을 내다보며 커피를 마시며 다가올 모험에 대한 생각에 잠겼다.",
#      "비에도 불구하고, 거리는 연례 음악 축제에 참석하려는 사람들로 북적였다."],  #1

#     ["버스를 기다리다가 몇 년 만에 본 친구를 우연히 만났어.",
#      "여름이면 가방을 싸서 할머니 집에 놀러 갔다.",
#      "스시를 처음 먹어볼 때, 궁금증과 조심스러움이 있었다.",
#      "그녀는 창가에 앉아 커피를 마시며 미래의 모험을 상상했다.",
#      "비가 오는데도 불구하고, 사람들은 음악 축제에 참여하기 위해 거리로 나왔다."], #2


#     ["버스 대기 중에, 오랜만에 본 친구를 우연히 만났다.",
#      "여름마다, 할머니댁을 찾기 위해 짐을 싸고 시골로 향했다.",
#      "스시를 처음 먹었을 때, 호기심과 약간의 걱정이 섞여 있었다.",
#      "그녀는 창문을 통해 바깥을 보며 커피를 마시고, 다가올 여행에 대해 생각했다.", #3
#      "비가 와도, 거리는 연례 음악 축제에 참가하려는 사람들로 붐볐다."],

#     ["버스를 기다리며, 몇 년간 못 본 친구를 만났다.",
#      "매년 여름, 할머니 집을 방문하기 위해 시골로 여행을 했다.",
#      "스시를 처음 먹었을 때, 호기심과 두려움이 있었다.",
#      "그녀는 창밖을 바라보며 커피를 마시며 미래의 모험을 상상했다.",
#      "비가 오는 날, 사람들은 음악 축제를 즐기기 위해 거리에 나섰다."],           #4

#     ["버스 정류장에서, 오랜 친구를 우연히 만났다.",
#      "여름 휴가 때마다, 할머니 집에 방문했다.",
#      "스시를 처음 접했을 때, 호기심이 가득했다.",
#      "그녀는 창가에 앉아 커피를 즐기며 모험에 대한 생각을 했다.",
#      "비에도 불구하고, 음악 축제를 즐기러 많은 사람들이 나왔다."],              #5

#     ["버스를 기다리는 중, 오랜만에 친구를 만났다.",
#      "매 여름, 할머니네 집을 방문하기 위해 여행을 했다.",
#      "처음 스시를 먹어봤을 때, 기대감과 긴장감이 있었다.",
#      "그녀는 창을 통해 밖을 바라보며 커피를 마시며 생각에 잠겼다.",
#      "비가 내리는 날씨에도, 음악 축제를 즐기기 위해 많은 사람들이 거리에 나왔다."], #6

#     ["버스를 기다리며, 몇 년 동안 못 본 친구를 만났다.",
#      "여름마다, 할머니의 집을 방문하기 위해 짐을 싸서 갔다.",
#      "스시를 처음으로 먹었을 때, 호기심이 가득 찼다.",
#      "그녀는 창가에 앉아 커피를 마시며 다가올 모험에 대해 꿈꾸었다.",             #7
#      "비가 오는데도, 거리는 음악 축제를 즐기기 위해 사람들로 붐볐다."],

#     ["버스를 기다리다가, 오랫동안 못 본 친구를 만났다.",
#      "여름마다, 할머니 집을 찾아가기 위해 짐을 싸고 떠났다.",
#      "스시를 처음 먹어보았을 때, 호기심과 두려움이 섞였다.",                    #8
#      "그녀는 창밖을 바라보며 커피를 마시고, 새로운 모험을 생각했다.",
#      "비가 와도, 사람들은 연례 음악 축제를 즐기기 위해 거리로 나왔다."],

#     ["버스를 기다리며, 몇 년 만에 친구를 우연히 만났다.",
#      "매 여름, 할머니 집을 방문하기 위해 시골로 여행을 갔다.",
#      "스시를 처음 먹었을 때, 호기심과 긴장감이 섞였다.",
#      "그녀는 창가에 앉아 커피를 마시며 미래의 모험을 상상했다.",
#      "비가 내리는데도 불구하고, 음악 축제를 즐기기 위해 사람들이 거리에 나섰다."],  #9

#     ["버스를 기다리는 동안, 오랫동안 못 본 친구를 만났다.",
#      "여름마다, 할머니 집을 찾기 위해 짐을 싸고 떠났다.",
#      "스시를 처음으로 먹어보았을 때, 호기심과 불안이 섞였다.",
#      "그녀는 창밖을 내다보며 커피를 마시며 다가올 모험을 생각했다.",
#      "비가 와도, 거리는 연례 음악 축제에 참석하려는 사람들로 북적였다."],        #10

#     ["버스를 기다리다가, 몇 년 만에 본 친구를 만났다.",
#      "여름마다, 할머니 집을 방문하기 위해 가방을 싸고 떠났다.",
#      "스시를 처음 먹었을 때, 호기심과 두려움이 섞였다.",
#      "그녀는 창밖을 바라보며 커피를 마시며 미래의 모험을 꿈꾸었다.",
#      "비가 오는데도 불구하고, 사람들은 음악 축제를 즐기기 위해 거리로 나왔다."],    #11

#     ["버스를 기다리는 동안, 오랫동안 못 본 친구를 만났다.",
#      "여름마다, 할머니 집을 찾기 위해 짐을 싸고 떠났다.",
#      "스시를 처음 먹어볼 때, 호기심과 불안감이 섞였다.",
#      "그녀는 창밖을 내다보며 커피를 마시며 다가올 모험을 상상했다.",
#      "비가 와도, 거리는 연례 음악 축제에 참석하려는 사람들로 붐볐다."],           #12

#     ["버스를 기다리다가, 몇 년 만에 본 친구를 만났다.",
#      "여름마다, 할머니 집을 방문하기 위해 가방을 싸고 떠났다.",
#      "스시를 처음 먹었을 때, 호기심과 두려움이 섞였다.",
#      "그녀는 창밖을 바라보며 커피를 마시며 미래의 모험을 꿈꾸었다.",
#      "비가 오는데도 불구하고, 사람들은 음악 축제를 즐기기 위해 거리로 나왔다."],      #13

#         ["버스를 기다리다가, 오랜 친구를 우연히 만났다.",
#      "여름에는 항상 할머니 집을 방문하기 위해 시골로 갔다.",
#      "스시를 처음 먹었을 때, 호기심과 불안이 섞였다.",
#      "그녀는 창가에 앉아 커피를 마시며 다가올 모험에 대해 생각했다.",
#      "비가 오는데도, 거리는 연례 음악 축제에 참석하려는 사람들로 북적였다."],         #14

#     ["버스를 기다리는 중, 오랜만에 친구를 만났다.",
#      "여름마다, 할머니네 집에 놀러 갔다.",
#      "스시를 처음 먹어봤을 때, 호기심이 가득했다.",
#      "그녀는 창밖을 바라보며 커피를 마시며 모험을 상상했다.",
#      "비가 와도, 사람들은 음악 축제를 즐기기 위해 거리에 나왔다."],                  #15

#     ["버스를 기다리다가, 몇 년 동안 못 본 친구를 만났다.",
#      "여름에는 할머니 집에 방문하기 위해 시골로 여행을 했다.",
#      "스시를 처음 먹었을 때, 호기심과 불안감이 섞였다.",
#      "그녀는 창밖을 내다보며 커피를 마시며 미래의 모험을 생각했다.",
#      "비에도 불구하고, 거리는 연례 음악 축제에 참석하려는 사람들로 북적였다."],       #16

#     ["버스를 기다리며, 오랜 친구를 우연히 만났다.",
#      "여름마다, 할머니 집을 찾아가기 위해 짐을 싸고 떠났다.",
#      "스시를 처음 먹어보았을 때, 호기심과 두려움이 섞였다.",
#      "그녀는 창밖을 바라보며 커피를 마시며 다가올 모험을 상상했다.",
#      "비가 와도, 거리는 연례 음악 축제에 참석하려는 사람들로 붐볐다."],              #17

#     ["버스를 기다리다가, 몇 년 만에 본 친구를 만났다.",
#      "여름마다, 할머니 집을 방문하기 위해 시골로 갔다.",
#      "스시를 처음 먹었을 때, 호기심과 긴장감이 섞였다.",
#      "그녀는 창밖을 바라보며 커피를 마시며 미래의 모험을 생각했다.",
#      "비가 내리는데도 불구하고, 사람들은 음악 축제를 즐기기 위해 거리로 나왔다."],    #18

#     ["버스를 기다리는 동안, 오랫동안 못 본 친구를 만났다.",
#      "여름마다, 할머니 집을 찾기 위해 짐을 싸고 떠났다.",
#      "스시를 처음 먹어볼 때, 호기심과 불안감이 섞였다.",
#      "그녀는 창밖을 내다보며 커피를 마시며 다가올 모험을 생각했다.",
#      "비가 와도, 거리는 연례 음악 축제에 참석하려는 사람들로 붐볐다."],               #19

#     ["버스를 기다리다가, 몇 년 만에 본 친구를 만났다.",
#      "여름마다, 할머니 집을 방문하기 위해 가방을 싸고 떠났다.",
#      "스시를 처음 먹었을 때, 호기심과 두려움이 섞였다.",
#      "그녀는 창밖을 바라보며 커피를 마시며 미래의 모험을 꿈꾸었다.",
#      "비가 오는데도 불구하고, 사람들은 음악 축제를 즐기기 위해 거리로 나왔다."]       #20

# ]

# top = [[0, 1, 2, 6, 12],  [0, 1, 2, 3, 4],  [0, 1, 2, 9, 12],  [0, 1, 2, 3, 9], [0, 1, 2, 3, 9]]

len(reordered)

reordered

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

line1 = [    "제 우산을 찾을 수 없습니다.","   어린 블로거로써 나비드는 그의 삶과 생각을 블로그를 통해 공유합니다.   .","     모든 것에도 불구하고 경찰은 떠나지 않습니다.   .","     하루에 학교를 방문할 수 있는 인원 수를 제한하는 정책이 있어야 합니다.   .","     이 행사의 주최자는 어디에 있나요?  .","      그들은 제대로 된 재활을 요구하고 있습니다.    "      ]

line1

winning = ["제 우산을 찾을 수 없습니다.",   "이 모든 것에도 불구하고, 그 경찰은 떠나지 않는다",  "하루에 학교 방문객 수를 제한하는 정책이 있어야 합니다", "이 행사의 주최자가 어디에 있나요?"]

# dataset = [“	나 우산을 못찾겠어”,”	Navie는 젊은 블로거로서, 자신의 삶과 아이디어를 블로그에 공유합니다.	”,”이 모든것에 불구하고 경찰은 떠나선 안됩니다.	”,”학교의 일별 방문자 수를 제한하는 정책이 마련되어야 합니다. ”,”	이 이벤트의 주체자는 어디에 있습니까? ”,”	그들은 적절한 재활을 요구합니다. “],
# [“ 나는 내 우산을 찾을 수 없다. 	”,”신입 블로거로서, 나비드는 그의 삶과 생각을 둘 다 그의 블로그에서 공유한다. ”,”	이 모든 것에도 불구하고, 그 경찰은 떠나지 않는다. ”,”	학교에서는 날마다 방문객의 숫자를 제한하는 정책이 있어야 한다. ”,”	이 행사의 조직자가 어디에 있나요? ”,”	그들은 적절한 재활을 요구하고 있다. “],
# [“	내 우산을 못 찾겠어. ”,”	젊은 블로거로서, 나비드는 그의 삶과 아이디어들 둘 다를 그의 블로그에  공유한다. ”,”	이 모든 것에도 불구히고, 경찰은 떠나지 않았다. ”,”	일일 학교 방뮨자를 제한하는 규칙이 있어야 한다. ”,”	이 행사의 주최자는 어디에 있습니까? ”,”	그들은 적절힌 재활을 요구하고 있다. “],
# [“	우산을 찾을 수 없어요.	”,” 네이비드(Navid)는 젊은 블로거로, 삶과 아이디어를 블로그에 공유합니다. ”,”	어떤 일이 있어도, 경찰은 그 자리를 떠나지 않는다.	”,”하루 학교 방문자 수를 제한하는 정책이 있어야 한다. ”,”	여기 이벤트 주최자 어디 있어요? ”,”	그들은 더 나은 부흥을 원한다.	“],
# [“	나는 내 우산을 찾을 수 없다	”,”젊은 블로거로서, Navid는 그의 삶과 생각 모두 그의 블로그이 공유한다. ”,”	이 모든 일에도 불구하고, 경찰관은 떠나지 않는다. ”,”	학교의 하루 방문자수를 제한하는 방침이 있어야만 한다. ”,”	이 행사의 주촤자는 어디에 있는가?	”,”그들은 정당한 복직을 요구하고 있습니다	“],
# [“	제 우산을 찾을 수 없습니다”,”	젊은 블로거로서, 네이비드는 그의 블로그에서 삶과 생각을 모두  공유합니다. ”,”	이 모든거에도  불구하고, 경찰은 떠나지 않습니다	”,”하루에 학교 방문객 수를 제한하는 정책이 있어야 합니다”,”	이 행사의 주최자는 어디에 있습니까? ”,”	그들은 적절한 재활을 요구하고 있습니다	“],
# [“	우산을 찾을 수 없습니다. ”,”	나비드는 초보 블로거로서 그의 일상과 생각을 블로그에 공유했다”,”	그럼에도 불구하고 경찰은 떠나지 않았다”,”	하루에 학교를 방문할 수 있는 방문자 숫자를 제한하는 규칙이 있어야 한다. ”,”	이 행사의 운영자는 누구인가요? ”,”	적절한 재활에 대한 수요가 있다	“],

# np.unique(labels, return_counts=True)
# medoids

# # find the cluster with largest elements

# def experiment(setences, num_cluster = 3, model_link = model_name):

#   # after the sentence is reordered
#   output = []
#   for index in range(len(sentences)):

#     temp = sentences[index]

#     dist_mat = dist_matrix(temp, model_link = model_link)
#     lab, med = cluster(dist_mat, num_cluster = num_cluster)

#     model = BertForMaskedLM.from_pretrained(model_link)
#     tokenizer = BertTokenizer.from_pretrained(model_link)

#     medoid_sentences = [temp[j] for j in med]
#     # print("medoid sentence is ", medoid_sentences)
#     prob_medoid_sent = []

#     for sent in medoid_sentences:
#       p = sentence_probability(sent, model, tokenizer)
#       print(f"Probability of the sentence '{sentence}' is: {p}")
#       prob_medoid_sent.append(p)
#       # [sentence_probability(sent, model, tokenizer) for sent in medoid_sentences]

#     prob_med_pair = list(zip(prob_medoid_sent, medoid_sentences))

#     output.append(prob_med_pair)

#   return output